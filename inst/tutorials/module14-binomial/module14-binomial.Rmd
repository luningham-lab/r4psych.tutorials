---
title: "Binomial regression in the generalized lienar model"
tutorial:
  id: "module14-binomial"
  name: "Binomial regression in R"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
author: Justin Luningham
description: "Binomial regression in R."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(MASS)
library(car)
library(emmeans)
library(afex)
library(kableExtra)
library(jtools)
library(infer)
library(moderndive)
library(learnr)
library(report)
library(rempsyc)
library(broom)
library(data.table)
library(flextable)
library(rstatix)
library(ggpubr)
library(tidyverse)
library(tutorial.helpers)
library(datarium)
library(GGally)
library(mosaicData)
options(repos = c(CRAN = "https://cran.rstudio.com"))
if (!requireNamespace("gradethis", quietly = TRUE)) {
  learnr::tutorial_warning(
    "This tutorial uses the **gradethis** package to provide feedback.  
    It looks like you donâ€™t have gradethis installed yet.  
    Please run this in your console:
    
    remotes::install_github('rstudio-education/gradethis')"
  )
} else {
  library(gradethis)
  gradethis::gradethis_setup()  # optional: standardizes grading defaults
}

set.seed(144445)
one_sample <- tibble(first_walk = round(rnorm(50, 12.8, 2.5),1))
raw_pairs<-mvrnorm(25, mu = c(14, 12.8), Sigma = cbind(c(2.5, 1), c(1, 2.5)))

paired_samples <- tibble(sib1 = round(raw_pairs[,1],1), sib2 = round(raw_pairs[,2],1), famID=1:25)

paired_long <-paired_samples |> pivot_longer(cols = c(sib1, sib2), values_to = "first_walk", names_to = "sibling")

two_sample<-tibble(control = round(rnorm(50, 14, 2.5),1), intervention = round(rnorm(50, 13, 2.5 ),1))

two_sample_long <- two_sample |> 
  pivot_longer(cols = c(control, intervention), # or, cols = everything()
              values_to = "first_walk", names_to = "group")

#dat <- read_delim("~/Downloads/Tab10-2.txt", 
#    delim = "\t", escape_double = FALSE, 
#    trim_ws = TRUE)

attr(stress$score, "label") <- "Stress Score"
attr(stress$age, "label") <- "Age"

#contrasts(stress$treatment) <- contr.sum(2)
#contrasts(stress$exercise) <- contr.sum(3)





fit2 <- glm(outcome ~ age, Whickham, family = "binomial")
effect_plot(fit2, pred = "age", interval = TRUE, plot.points = TRUE)
effect_plot(fit2, pred = "age", interval = TRUE)


fit3 <- glm(outcome ~ smoker, Whickham, family = "binomial")
summary(fit3)


fit4 <- glm(outcome ~ smoker + age, Whickham, family = "binomial")



#fit5 <- glm(outcome ~ smoker + age_cat, Whickham, family = "binomial")

```


```{r info-section, child = system.file("child_documents/info_section.Rmd", package = "tutorial.helpers")}
```


## Categorical outcomes
###

All of the models we've learned so far assume that the outcome is a continuous, quantitative variable with errors that have a mean of 0 and a constant variance. In many cases, we also assume that these errors are normally distributed, which means that $Y$ conditioned on $X$ is also normally distributed. 

Of course, many outcomes that we study are not exactly continuous and are certainly not normally distributed. We mentioned that many of these approaches (e.g. t-test, ANOVA, even linear regression) are **robust** to some violations of assumptions. They are still accurate even if data are not perfectly normal or variances are not equal across groups. 

Will this hold if the outcome is not truly continuous? 

###

As usual, the answer is "it depends!" How different from continuous is our outcome? 

If the outcome is a Likert-type variable on 1-7 scale with $\sim$ 500 responses, it might be approximately normal. But what if the outcome is a specific **event**, such as answering a question correctly or receiving a diagnosis?

###

Let's revisit the `Whickham` dataset examining smoking status and mortality among Women in Whickham, UK. The dataset is a subset of women who were current smokers or never smokers between 1972-1974, and their survival status was ascertained after 20 years. The variables are `outcome`, `smoker`, and `age` at the time of first survey. 

```{r fit1, exercise = TRUE}
library(mosaicData)
glimpse(Whickham)
```

###

Fit a simple linear model predicting `outcome` from `age` below. Call it `fit1`. 

```{r fit2, exercise = TRUE}
fit1 <- ___(___, Whickham)
```

###

The warning message occurs because `lm` struggles internally when the outcome is a factor, not a number. Let's quickly create a new column called `outcome2`, which converts `outcome` to a numeric value. 

```{r fit3, exercise = TRUE}
Whickham <- Whickham |>
  ___(outcome2 = (as.numeric(___) - 1))
Whickham |> count(outcome, outcome2) #check
```


```{r fit3-2, include = FALSE}
Whickham <- Whickham |> mutate(outcome2 = as.numeric(outcome)-1)
```

I include the `-1` part here to scale `outcome2` as 0 or 1. 

###

Now, re-run `fit1` with `outcome2` as the outcome. Check the `effect_plot` with `age` as the predictor. Be sure to plot the raw data points.  

```{r fit4, exercise = TRUE, exercise.setup = "fit3-2"}
fit1 <- lm(outcome ~ age, Whickham)

___(fit1)
effect_plot(fit1, pred = ___, 
            interval = ___, 
            plot.points = ___)
```

```{r fit4-2, include = F}
Whickham <- Whickham |> mutate(outcome2 = as.numeric(outcome)-1)
fit1 <- lm(outcome2 ~ age, Whickham)
```

###

Let's examine the residuals against the fitted values. `augment()` the fit and pipe it to a simple `ggplot` for residuals on the `y` axis and fitted values on the `x` axis. 

```{r fit5, exercise = TRUE, exercise.setup = "fit4-2"}
___(fit1) |> 
  ggplot(aes(x = .fitted, y = .resid))+
  geom____()
```

###

Yikes!

### The generalized linear model 

Clearly, there are instances where a linear predictor of a continuous $Y$ simply does not make sense. Statistically, it's a mess, and practically, it's impossible to interpret. An "event" outcome is a clear example. 
With a binary outcome, the observed data are simply zeros and ones, and they follow a specific distribution called a Bernoulli distribution. This is a special case of the binomial distribution with only one "trial". The parameter of the Bernoulli distribution is $p$, of the probability that the outcome equals 1 (probability of "success"). 

###

For example, a single coin flip follows a Bernoulli distribution with $p = 0.5$. If 1 = Heads, then $P(Y = Heads)$ = 0.5 and $P(Y = Tails) = 1 -P(Y = Heads) = 0.5$. 

In our regression model, we don't want to model the 0's and 1's directly; instead, we explicity model $P(Y = 1)$, or $p$. 

###


The **generalized** linear model extends the general linear model by applying some "link function" to the outcome, such that it is transformed in a way that corresponds to the unique distribution of the data. 

$$ g(Y) = \beta_0 + \beta_1 X_{1} + \beta_2 X_{2} +... +\beta_k X_k$$ 
The key take-aways are that

1. The outcome is still "linear in the predictors" - effects are additive
2. However, we have to interpret the regression coefficients as a function of the *inverse* of the link function
3. The errors/residuals are not included because they look different depending on the form of the link 

###

For binary outcomes, the probability that $Y = 1$, or $p$, is linked to the outcomes through the log-odds link:

$$ \log (\frac{p}{1-p}) = \beta_0 + \beta_1 X_{1} + \beta_2 X_{2} +... +\beta_k X_k$$
We also refer to this outcome as the "log odds" of $Y = 1$.

###

This all happens internally with our statistical software; however, the estimates are usually on the un-transformed scale. The inverse of the log odds is to take the exponential of everything on the right-hand side. 

###

Note: the the log-odds "link" is technically called the *logit* link, so you may hear that terminology. The logit link is the inverse of the logistic function, which is applied to the linear predictor to keep predicted values between 0 and 1 (probabilities). For this reason, this approach is often referred to as *logistic regresion*. 

###

Enough of that, let's see what this means in practice. The base `R` function for the generalized linear model is conveniently called `glm()`. It operates a lot like the `lm`, but it requires an additional argument called `family`. The `family` refers to the type of distribution of the outcome. For 0/1 data, we add `family = "binomial"`. 

The following fits the model and gets the summary, in the same way as `lm` - with slightly different information.

```{r fit6, exercise = TRUE, exercise.setup = "fit4-2"}
fit2 <- glm(outcome ~ age, Whickham, 
            family = "binomial")
summary(fit2)
```

### Interpretation - effect estimate

In `fit1`, the age coefficient was 0.017, or for every year increase, the predicted increase on outcome was 0.017. Now the age estimate is 0.12. What does this mean? 

Recall that this estimate is on the untransformed scale. One year increase in age corresponds to 0.12 increase in the log-odds of death. For these coefficients, a positive value indicates an increase in the predictor leads to an increase likelihood of the outcome, whereas a negative coefficient suggests a decrease in the likelihood.

If we take the inverse of the log, we get something much more interpretable: the odds ratio. This is the multiplicative change in the odds of the outcome per one-unit change in the predictor. No effect is indicated by odds ratio of 1.0. Greater than 1 indicates an increase in odds, less than one indicates a decrease in the odds. The inverse is found by "exponentiating" the regression coefficients. 

###

Here is a little math behind it. From the equation above: 

$$ \log (\frac{p}{1-p}) = \beta_0 + \beta_1 X_{1} + \beta_2 X_{2} +... +\beta_k X_k$$
where $p = Prob(Y = 1)$ and $1 - p = Prob(Y = 0)$. Exponentiate both sides:

$$  \frac{p}{1-p} = \exp(\beta_0 + \beta_1 X_{1} + \beta_2 X_{2} +... +\beta_k X_k)$$
The left-hand term is also called the "odds", the probability of the event happening divided by probability of event not happening.

###

Let's compare what happens when we increase $X_1$ by 1 unit, but instead of taking he difference, we'll look at the ratio of the two predictions: 

$$ \frac{\text{odds}_{x_1 + 1}}{\text{odds}_{x_1}} = \frac{\exp(\beta_0 + \beta_1 (X_{1} + 1) + ... +\beta_k X_k)}{\exp(\beta_0 + \beta_1 X_{1} + ... +\beta_k X_k)}$$

### 


This mathematical rule applies here: $$\frac{ \exp(a)}{\exp(b)} = \exp(a - b)$$ 

so we have the numerator minus the denominator from the ratio of the odds for $x_1 + 1$ and $x$. Many terms cancel out, and we are left with:

$$ \frac{\text{odds}_{x_1 + 1}}{\text{odds}_{x_1}} =\exp(\beta_1 (X_{1} + 1) - \beta_1 X_{1} ) = \exp(\beta_1)$$
In the end, $\exp(\beta_j)$ equals the ratio of the odds ("odds ratio"!) of $X_j$ increasing by one unit. 

###

In `R`, we can use the function `exp()` on the original coefficients to obtain the odds ratio. 

More conveniently, we can `tidy()` our output and ask for the transformation internally with `exponentiate = TRUE`. 

```{r fit7, exercise = TRUE, exercise.setup = "fit6"}
tidy(fit2, exponentiate =  TRUE, conf.int = TRUE)
```

This means that every one-year increase in age leads to a 13% increase in the odds of death. We could also say that the odds of death increase by a factor of 1.13 per one-year increase in age. 

###

Note that a 2-unit increase is not $2\times 1.13 = 2.26$ change in the odds. The odds increase would increase by a factor of $ 1.13^2 = 1.13 \times 1.13 = 1.28$. 

### Interpretation - inference

Let's look again at the original `summary` for fit2:

```{r fit7-2, exercise = TRUE, exercise.setup = "fit6"}
summary(fit2)
```

As before, the $p$-value is based on the null hypothesis $H_0: \beta = 0$.

The $p$-value is extremely small. This is the probability of obtaining an observed test statistic at least as large as we have here, given that the null hypothesis is true. 

The test statistic is calculated in the usual way: $$\frac{\text{estimate}}{\text{std. error of estimate}}$$

which is $z = \frac{.12}{.007} = 17.6$ here. 

### 

The `effect_plot` function works with logistic regression seamlessly. Below is our usual `effect_plot` code with `fit2`:

```{r fit8, exercise = TRUE, exercise.setup = "fit6"}
effect_plot(fit2, 
            ___ = "age", 
            interval = ___)
```


###

The intercept is the odds of the event happening when all $X$ variables equal 0. The intercept is usually not meaningful unless the $X$ values have been intentionally scaled (e.g., all centered at their means). Here, the intercept is the odds of death at 20-year followup for newborns (age = 0). The odds ratio of the intercept is 1.0006, meaning there is no difference in the odds of death. However, we didn't actually measure any newborns in the data, so it's hard to draw conclusions about this. 

The intercept is usually not of interest in logistic regression.

### Categorical Predictors

Now let's fit a model with smoking status as the only predictor. Call the result `fit3`. 

```{r fit9, exercise = TRUE, exercise.setup = "fit6"}
fit3 <- ___(outcome ~ smoker, 
            Whickham, 
            family = ___)
summary(fit3)
tidy(fit3, ___)
```

```{r fit9-2, include = F}
fit3 <- glm(outcome ~ smoker, 
            Whickham, 
            family = binomial)
```

###

```{r quiz1, echo = FALSE}
question_text("The **OR** for `smoker == Yes` is 0.685. How do we interpret this again?",
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

### Interpretation - categorical predictors

Here we have a categorical predictor. The OR is the odds of death for the specific category of $X$ relative to the reference category. The reference category is non-smoker. So, being a smoker compared to non-smoker means the odds of death are reduced by a factor of 0.685. 

This can also be interpreted as a percent reduction: (1 - 0.685)*100 = 31.5% reduction in the odds of death for smokers compared to non-smokers. 

###

We can see model-predicted probabilities of "event" with the `effect_plot`: 

```{r fit10, exercise = TRUE, exercise.setup = "fit9-2"}
effect_plot(fit3, pred = ___)
```

Because this is a simple model without other predictors, the model-implied probabilities align with descriptive probabilities: 

```{r fit11, exercise = TRUE}
Whickham |> count(outcome, smoker) |>
  group_by(smoker) |> 
  mutate(prop = n/sum(n))
```

### Multiple Logistic Regression

Let's now consider multiple predictors in the model, as we normally would in practice. 

Add both `smoker` and `age` to the model below, and call it `fit4`. 

```{r fit12, exercise = TRUE}
fit4 <- ___(___, ___, family = ___)
summary(fit4)
tidy(fit4, ___=TRUE)
```

```{r quiz2, echo = FALSE}
question_text("Interpret the effect of age and smoking status in this model.",
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

###

Controlling for smoking status, the age effect is largely unchanged - the OR = 1.13 and the test statistic is roughly the same. 

Controlling for age, being a smoker is now associated with a 23% *increase* in the odds of death. Substantively, this makes a lot more sense. However, coefficient for `smoker == Yes` is no longer significant, so we can't say for certain that there is any difference in the probability of death based on smoking status. 

### 

Similar to linear regression, we also want to assess the adequacy of model fit and whether adding variables leads to better model fit. 

Logistic regression (and GLM generally) cannot rely on least squares estimates and the $F$ test. Think about why: 

the residual is $y - \hat{y}$. In logistic regression, $y$ is either 0 or 1, and we actually don't estimate $\hat{y}$ directly - we estimate $\hat{p}$, the probability that $Y = 1$. 

### 

Instead, logistic regression is based on **maximum likelihood estimates**, and model fit is assessed by something called the *Deviance*. Deviance is the difference in the (log) likelihood between the regression model of interest and something called a *saturated* model, or a model with as many parameters as observations. This model fits the sample data perfectly. 

Deviance is then a measure of deviation from a perfect model. By itself, it is not incredibly meaningful. However, the difference in deviance between two nested models follows a $\chi^2$ distribution, so we can use this to obtain a **likelihood ratio test** (LRT)!

###

Here is an example of the LRT. Let's see if adding `age` to the simple model with `smoker` improves model fit. 

```{r fit13, exercise = TRUE}
anova(fit3, fit4)
```

### 

The message indicates that it is performing an analysis of "deviance" rather than variance. If we add `test = "LRT"`, the result is the exact same.

The significant result tells us that the additional parameters of the larger model significantly improve model fit.

###

For future reference, we can clean up the output with `tidy` and `nice_table`:

```{r fit14, exercise = TRUE}
anova(fit3, fit4) |>
  tidy() |>
  nice_table()
```

## Classification Problems
###

Logistic regression is widely used for purposes other than inference. In particular, we may be interested in classification, or predicting which group/categorical outcome a person is expected to fall in, given a set of predictor variables. 

Again, we are not predicting 0's or 1's directly in the logistic regression model: we are modeling $p = P(Y = 1)$. How might we convert $\hat{p}$ to $\hat{y}$? 

###

We can impose a decision rule, such that if someone's probability is above a certain threhold, they are assigned $\hat{y} = 1$, otherwise $\hat{y} = 0$. This is referred to as "hard assignment" from a probabilistic model. 

Except in very special circumstances, the decision rule for binary classification is 
$$
\hat{y_i} = 
\begin{cases}
1 & \text{ if  } \hat{p} > 0.5, \\\\
0 & \text{ otherwise} 
\end{cases}
$$

###

This use case is another example of why logistic regression is valuable and why linear regression fails in these problems. 

Linear regression does predict values between 0 and 1, but 1) it is not forced to center at 0.5, and 2) it can go beyond 0 or 1. 

![Courtesy of Christoph Molnar, *Interpretable Machine Learning*:](https://christophm.github.io/interpretable-ml-book/logistic_files/figure-html/fig-linear-class-threshold-1.png)

### 

By applying the logistic function to the linear predictor, it is forced to fall between 0 and 1, and 0.5 is 50/50 probability:

![Courtesy of Christoph Molnar, *Interpretable Machine Learning*:](https://christophm.github.io/interpretable-ml-book/logistic_files/figure-html/fig-logistic-class-threshold-1.png)

###

We don't get $\hat{y}$ out of `glm` directly, but we can obtain it in a few steps. Revisting our `fit4` object, we can use the `predict()` function. 

```{r classify1, exercise = TRUE}
predict(fit4) |> head(10)
```

###

Hm. These don't look like probabilities!

The default output is on the scale of the linear predictions of the $X$ variables, which is the predicted log-odds. Thankfully, getting the probabilities is easy if we ask for the predictions on the "response" scale with `type = "response"`.

```{r classify1, exercise = TRUE}
predict(fit4, type = "response") |> head(10)
```

###

This returns $\hat{p}$. We need to apply a classification decision rule to obtain $\hat{y}$. This comes down to a little bit of programming that we've seen before: `ifelse` or `if_else` functions. 

```{r classify2, exercise = TRUE}
phat<-predict(fit4, type = "response")

ifelse(phat > .5, "Dead","Alive") |> head(10)
```

### 

A helpful function that we haven't seen before is called `table()`. It is similar to the `count()` function of `dplyr`, but it works with matrices and not only `data.frames`. 

```{r classify3, exercise = TRUE, exercise.setup = "classify2"}
ifelse(phat > .5, "Dead","Alive") |> table()
```

Compare this to the original data by `count`ing the `outcome`:

```{r classify4, exercise = TRUE, exercise.setup = "classify2"}
Whickham |> ___(outcome)
```

### 

We cans see the predicted and original classifications don't perfectly align. A common way to summarize this is through a *confusion matrix*: a $2 \times 2$ table that includes predicted 0 and 1 as one factor and true 0 and 1 as the other factor. 

`table()` is one tool to create this. Below, look at each line and interpret the steps I take: 

```{r classify5, exercise = TRUE, exercise.setup="classify2"}
Whickham <- Whickham |>
  mutate(phat = predict(fit4, type = "response"),
         yhat = ifelse(phat > .5, "Dead", "Alive"))

table(Truth = Whickham$outcome, Predicted = Whickham$yhat)
```

### 

Next time: 

- interpreting the confusion matrix 
- visualizing false positive and true positive 
- summarizing prediction accuracy 


```{r download-answers, child = system.file("child_documents/download_answers.Rmd", package = "tutorial.helpers")}
```
