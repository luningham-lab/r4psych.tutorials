---
title: "Crash course on t-tests in R"
tutorial:
  id: "module10-t-tests"
  name: "Performing t-tests in R"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
author: Justin Luningham
description: "Performing t-tests in R."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(MASS)
library(tidyverse)
library(infer)
library(moderndive)
library(learnr)
library(report)
library(rempsyc)
library(broom)
library(data.table)
library(flextable)
library(rstatix)
library(ggpubr)
library(tutorial.helpers)
options(repos = c(CRAN = "https://cran.rstudio.com"))
if (!requireNamespace("gradethis", quietly = TRUE)) {
  learnr::tutorial_warning(
    "This tutorial uses the **gradethis** package to provide feedback.  
    It looks like you don’t have gradethis installed yet.  
    Please run this in your console:
    
    remotes::install_github('rstudio-education/gradethis')"
  )
} else {
  library(gradethis)
  gradethis::gradethis_setup()  # optional: standardizes grading defaults
}

set.seed(144445)
one_sample <- tibble(first_walk = round(rnorm(50, 12.8, 2.5),1))



raw_pairs<-mvrnorm(25, mu = c(14, 12.8), Sigma = cbind(c(2.5, 1), c(1, 2.5)))

paired_samples <- tibble(sib1 = round(raw_pairs[,1],1), sib2 = round(raw_pairs[,2],1), famID=1:25)

paired_long <-paired_samples |> pivot_longer(cols = c(sib1, sib2), values_to = "first_walk", names_to = "sibling")

two_sample<-tibble(control = round(rnorm(50, 14, 2.5),1), intervention = round(rnorm(50, 13, 2.5 ),1))

two_sample_long <- two_sample |> 
  pivot_longer(cols = c(control, intervention), # or, cols = everything()
              values_to = "first_walk", names_to = "group")
```


```{r info-section, child = system.file("child_documents/info_section.Rmd", package = "tutorial.helpers")}
```


## t-test overview
###

One of the most common tests in statistics, the t-test, is used to determine whether the means of two groups are equal to each other. The assumption is that both groups are sampled from normal distributions with equal variances. 

The null hypothesis is that the two means are equal, and the alternative is that they are not. Sometimes, we may specify a particular direction in the alternative hypothesis (e.g., the treatment group has lower cholesterol scores than placebo in a clinical trial of a drug that reduces cholesterol). 

###

Let's consider a motivating example throughout this module. The average age that children start walking is 14 months old. Three researchers propose that intensive music therapy leads to earlier walking times in children. The three researchers design three different studies to test this therapy.

Researcher 1 will follow one group of babies given the therapy regimen. Their age at first walking is recorded. The researcher will compare the average walking time of this group to the average age in the population. 

###

Researcher 2 points out that this design above is too susceptible to chance, and the individual differences in walking time are highly heritable. To control for this, the researcher will study siblings from the same family – one who receives the therapy and one who does not. The age at first walking is recorded for both siblings. This partially controls for genetic differences. 

###

Researcher 3 notes that siblings are raised by the same families, so the shared environment may mitigate or exacerbate the effects of the therapy. They propose a true randomized trial in which a larger group of children are randomly assigned to receive the therapy or placebo, and these two groups are compared.

###

All three of these designs involve making mean comparisons. Does anyone recall the three **types** of t-tests that one can conduct?

```{r quiz1, echo = FALSE}
question_text("The three types of t-tests are___",
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

### 

Before we proceed, let's consider the logic of hypothesis testing for mean comparisons. 

Suppose we conducted the Researcher 1's study above following $N = 50$ children exposed to the music therapy regimen. These children had an average age of first walking of 13.1 months. This sample mean is less than the population mean of 14 months, but is this a real effect? Or did this happen by chance? 

In other words, how plausible is it to observe a sample mean of 13.1 in 50 children if the true mean is 14? We are back to the null hypothesis testing framework! 

###

Previously, we learned about the sampling distribution of a proportion. There, the outcome was a discrete variable - the number of outcomes per a fixed number of samples (e.g., complications per 62 patients). We could calculate the probabilities of specific outcomes. 

![](images/binom_prob_mass.png)
![](images/area_under_binom_curve.png)

###

The same principle applies to continuous outcomes. How do we know? 

Recall the core principles of the sampling distribution of the mean: it is normally distributed with sufficiently large samples, with a mean equal to the population mean and standard deviation that is $\frac{\sigma}{\sqrt{N}}$. 

### 

The *normal distribution* follows what's called a probability density function. When a random variable can be represented by a density curve, the probability that the random variable takes a value in any given interval (on the X-axis) is equal to the area under the density curve for that interval.

![](images/normal_curve.png){width=70%}


### 

For our example, we need to construct the sampling distribution under the null hypothesis. 

If the population average walking time is 14 months, then walking time in general is normally distributed with $N(14, \sigma)$ and the sampling distribution of average walking times taken from repeated samples is $N(14, \sigma/\sqrt{N}$ 

But what is $\sigma$? 

### 

We don’t know! Instead, we use the estimated sample standard deviation as a stand-in for population standard deviation. However, this adds some uncertainty to our sampling distributionm so we change the shape of our sampling distribution to be more conservative in our hypothesis test. 

Instead of a normal distribution, it follows a t-distribution, which is like a normal distribution but with more area in the tail ends depending on the sample size (or, degrees of freedom). 

```{r, echo = F, warning = F, message = F}
x_range <- data.frame(x = c(-4, 4))


ggplot(x_range, aes(x = x)) +
  stat_function(fun = dt, args = list(df = 5), aes(colour = "t (df=5)")) +
  stat_function(fun = dt, args = list(df = 20), aes(colour = "t (df=20)")) +
  stat_function(fun = dnorm, aes(colour = "Normal"), linetype = "dashed") +
  labs(title = "Comparison of t-distributions and Normal Distribution",
       x = "x-value",
       y = "Density",
       colour = "Distribution") +
  theme_minimal()
```

The degrees of freedom equal the total sample size minus the number of things we estimate. For a one-sample test, we estimate one mean, so DF = N – 1. 

### 

Back to our example. Suppose that our sample standard deviation was 2.5 and the null hypothesis is $\mu = 14$. Then, the sampling distribution of $N = 50$ samples follows a distribution of $N(14, 2.5/\sqrt{50})$: 

```{r, echo =F, warning = F, message = F}
x_range <- data.frame(x = c(12, 16))

# Create the plot using stat_function for each distribution
ggplot(x_range, aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean=14, sd = 0.3535), aes(colour = "Normal")) +
  labs(title = "Comparison of t-distributions and Normal Distribution",
       x = "sample means",
       y = "Density",
       colour = "Distribution") +
  geom_segment(x = 13.1, xend = 13.1, y = 0, yend = .041) +
  annotate(x = 12.7, y = 0.07, geom = "text", parse = TRUE, label = "bar(x) == 13.1")+
  theme_minimal()
```

### 

We can calculate the probability under the curve of observing a $\bar{x} \leq 13.1$ under the null. This is the interpretation of the **p-value** for our test! 


## Executing t-tests
###

The prior page describes what goes on "under the hood" for our t-tests. Let's actually run some tests. 

Recall that for researcher 1, we have a single sample of 50 children, so we conduct a one-sample test. The one-sample tests are less common in practice, but our researcher here feels pretty confident that the general population mean for starting walking is 14 months. 

### 

The code for t-tests in `R` is pretty straightforward: we can execute a one-sampe with the base function `t.test()`, which takes the following arguments: 


| Argument       | Description         |
| ------ |---------------| 
| `x`     | A vector of data whose mean you want to compare to the null hypothesis `mu` | 
| `y`     | **Optional** second vector of data for paired or two-sample tests | 
| `mu`     | The population mean under the null hypothesis. For example, `mu = 0` will test the null hypothesis that the true population mean is 0.     |  
| `alternative` | A string specifying the alternative hypothesis. Can be `"two.sided"` indicating a two-tailed test, or `"greater"` or `“less"` for a one-tailed test.      |  
| `paired` | `TRUE` or `FALSE`. `TRUE` if conducting a paired samples t-test. Both vectors must be ordered by pairs. Default is `FALSE`     |  

###

The data are already in the tutorial environment and called `one_sample`. First, glimpse the data.

```{r test1, exercise = TRUE}
___(one_sample)
```

### 

Age when walking is called `first_walk`. Because `t.test()` takes a vector and not a dataset, we can extract that variable using the `$` operator. To signal that this is a one-sample test, we provide the population mean under the null hypothesis. 

```{r test2, exercise = TRUE}
t.test(one_sample$first_walk, 
       mu=___)
```

###

The output is not a numerical summary, but more of an explanation with text and numbers combined. The `t` is called the test statistic, it is the standardized value of how different our sample mean is from the null mean. `df` is the degrees of freedom, determining the shape of the `t` distribution. The `p-value` is the probability of seeing a sample mean of 12.75 if the null is true. The output spells out the alternative hypothesis and also provides a 95% CI for the sample mean. 

### Making Nice Output

This output is useful, but we will look at some additional tricks for creating publication-ready outputs here. We will use a package called `broom`, which creates "tidy" output from a wide range of analyses, and the package `rempsyc`: 

Thériault, R., (2023). rempsyc: Convenience functions for psychology. *Journal of Open Source Software*,
  *8*(87), 5466. https://doi.org/10.21105/joss.05466 

We will only use the `nice_table` output from this package, but it has many other convenience functions. 

### 

First, load the `broom` library, then wrap our `t.test` code from before in the function `tidy()`:

```{r test3, exercise = TRUE}
library(___)
t.test(one_sample$first_walk, mu=14)
```

We get similar information, but it is organized into columns of a results table. Quite nice! 

However, if I'm making a publication-ready table, I want to clean up some of the names. Use `rename()` from `dplyr` to fix a few column names. Let's be sure to assign the result as well: 

```{r test4, exercise = TRUE}
clean_res <- tidy(t.test(one_sample$first_walk, mu=14)) |> 
  rename(`Sample mean` = estimate, `t-statistic` = statistic, DF = parameter )
clean_res
```

### 

Now we will pipe this to a function called `nice_table()` after loading `rempsyc`. 

```{r test5, exercise = T, exercise.setup = "test4"} 
library(___)
clean_res |> nice_table()
```

### Exporting to Word

You could use `select` to remove or re-order the columns as needed. 

We can also export the nice table directly to Word. 

```{r test6, exercise = TRUE, exercise.setup = "test4"}
nice_res <- clean_res |> nice_table()
print(nice_res, preview = "docx")

# Save in Word
flextable::save_as_docx(nice_res, path = "one_sample.docx")
```

### Paired samples

Moving on to researcher 2. They are studying 50 siblings paired within the same family (25 pairs, i.e. 25 families), with one sibling randomly assigned to receive the music therapy. (Ignore the logistical nightmare of actually executing this study prospectively...)

This researcher will used the paired-samples t-test. While we still have 50 observations, the observations are not independent. However, the 25 different *pairs* are independent from each other. 
###

Let's re-think our null hypothesis ($H_0$) for this design. If the null is true, then the average time of first walking should be the same for both siblings in the pair, regardless of receiving therapy or not. If the alternative is true, then they are not equal.

However, we cannot average the music therapy group and the non-therapy group separately to compare their means because the observations are explicitly paired. They are not independent.

### 

In the unique case of paired samples, we need to reconsider the data itself. If there is no effect of therapy, then the mean difference between the intervention and control sibs in a pair should be 0. Any observed difference between them will be due to random factors. Further, it is random if the treated sibling walks sooner or later than their pair, so the random differences balance out in the long run. 

###

In this case, we can calculate a difference score $d$ for each pair, and the null hypothesis is

$$H_0: \mu_d = 0 $$

###

In other words, for the paired samples t-test, we conduct the test on difference scores. The sampling distribution used for calculating the p-value is the distribution of average difference scores across samples, and the standard error is $SD_{d}/\sqrt{N_{pairs}})$

###

To run in `R`, we provide two different vectors to the `t.test()` function, and we don't need to specify the population `mu` value. We add the extra argument `paired = TRUE`. Both vectors must be in the same order for the pairs to be aligned. 

The data are called `paired_samples`. First, glimpse the data.

```{r paired1, exercise = TRUE}
glimpse(___)
```

###

The two vectors are called `paired_samples$sib1` and `paired_samples$sib2`. It's important to note that in these data, `sib1` is the control sibling and `sib2` is the intervention sibling. Even though the order was randomized within family, the intervention and control observations must be correctly aligned in the data.

###

Before we test, let's conduct some exploratory data analysis. Calculate the mean and SD for each sibling. 

```{r summ_stat, exercise = TRUE}
paired_samples |> 
  ___

```

Add the two vectors for analysis to the `t.test` function with `paired = TRUE`. 

```{r paired2, exercise = TRUE}
t.test(___)
```

###

Once again, let's create a `tidy` result and output it with `nice_table`. 

```{r paired3, exercise = TRUE}
clean_pairs <- tidy(t.test(___))|> 
  rename(`Sample mean` = estimate, 
         `t-statistic` = statistic, 
         DF = parameter ) |> select(-alternative)

clean_pairs |> nice_table() 
```

### 

```{r quiz2, echo = FALSE}
quiz(question_text("Our siblings all walked around 13-14 months, give or take. Why does the analysis report a sample mean of 1.85?",
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3),
	question_text("How do we interpret the p-value and CI results?",
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
)
```

### Independent Samples 

Finally, consider the study conducted by Researcher 3. This researcher randomly assigned 50 children to music therapy and 50 to no music therapy groups. The null hypothesis is that there is no difference in the average walking time across the two groups. 

Under the null hypothesis, both groups are randomly drawn from the same general population, and any differences between the groups are just due to sampling fluctuation and random chance. The alternative hypothesis states that the group means are not equal because the music therapy children come from a different population than the non-therapy children. 

### 

This sounds a lot like the same setup for the sibling pairs, so does the test internally create difference scores between groups and test if the difference scores are equal to zero? 

### 

We cannot create difference scores in this design because the groups are truly independent. There is no logical way to pair up observations from one group with the other group - it would be arbitrary. Instead, we test if the overall difference in means is equal to zero. The null hypothesis is 
$$ H_0: \mu_1 = \mu_2$$ 
or equivalently 
$$ H_0: \mu_1 - \mu_2 = 0$$ 

###

One issue with the independent samples design is that because we have two different samples, there are different ways we can estimate the sample standard deviation: 

1. weighted average of the standard deviations (pooled SD)
2. estimate standard deviations in each group separately (standard error of the mean difference) 

The shape of the sampling distribution under the null hypothesis changes depending on which approach we use, which reflect **whether or not we assume that the groups have equal variance in the population**. 

###

The default method in `R` does **not** assume equal variances. It uses an approach called Welch's t-test, which adjusts the degrees of freedom through a complex formula that results in non-integer values for the DF. Essentially, as the groups variances become more different, the degrees of freedom get smaller, making the test more conservative (i.e., more extreme sample mean difference needed to reach statistical significance). 

### 

We haven't touched on assumptions much to this point. We will do so for this final example. For the two-sample t-test, we assume that 

1. The observations are truly independent
2. The data for each group are approximately normally distributed (or, we have large enough samples in each group such that the sampling distribution is normal)
3. The outcome has equal variance across groups

### 

The first assumption comes down to sampling and study design. The other two are often checked visually. 

Normality can be inspected with distributional plots, such as a histogram. The data are available in the data frame `two_sample`: 

```{r ind1, exercise = TRUE}
glimpse(two_sample)
```

### 

The data are in wide format. We can create two different geoms to put them together:

```{r ind2, exercise = TRUE}
two_sample |> 
  ggplot(aes(x = control))+
  geom_histogram(binwidth=.7, fill = "coral", color = "black", alpha = .6) + 
  geom_histogram(aes(x = intervention), binwidth = .7, 
                 color = "black", fill = "lightblue", alpha = 0.6)+
  labs(x = "Walking times", y = NULL)+ 
  theme_minimal()
```

### 

Is there another way we could format the data to create this plot?

### 

`ggplot2` has a couple of built-in plots that we have not yet discussed, but they are shortcuts to widely-used plots in mean comparison and regression situations. One is called `ggqqplot()`. 

A Q-Q plot is a way to inspect for normality. It orders the standardized values of the data and compares that to the values of a standardized normal distribution. If the data are normal, they should follow a straight line compared to the theoretical normal. 

###

First, we need to create a long format dataset. Then we use `ggqqplot`. It requires a dataset and variable names of the data, rather than explicitly mapping aesthetics as we usually do with `ggplot`. 

Here is how to use it: 

```{r ind3, exercise = TRUE}
two_sample_long <- two_sample |> 
  pivot____(cols = c(control, intervention), # or, cols = everything()
              values_to = "first_walk", names_to = "group")

ggqqplot(two_sample_long, x = "first_walk", facet.by = "group")
```

### 

The control group is approximately normal except for the highest and lowest points. I wouldn't say these qualify as outliers - they are just somewhat unexpected due to chance. 

Another built-in ggplot is a multi-group boxplot. This is accessed by `ggboxplot()`. 

```{r ind4, exercise = TRUE}
# minimal required arguments
ggboxplot(two_sample_long, x = "group", y = "first_walk") 

#making it slightly nicer
ggboxplot(two_sample_long, x = "group", y = "first_walk",
          ylab = "Age at first walking", add = "jitter",
          color = "group")
```

###

Finally, we can explicitly test the equality of variances with a Levene's test. The test is available from the `rstatix` package. Note that the null hypothesis here is that variances are equal; a significant p-value indicates a difference between groups. 

```{r ind5, exercise = TRUE}
library(rstatix)
two_sample_long |> levene_test(first_walk ~ group)
```

In practice, this test by itself is not always reliable. It can be too sensitive with really large sample sizes. 

###

According to the test, we could assume equal variances.

Finally, we will conduct the independent samples t-test. This can actually be carried out in either wide or long format. 

```{r ind6, exercise = TRUE}
#wide form: provide two vectors
t.test(two_sample$control, two_sample$intervention)

#long form: provide a formula & data
t.test(first_walk ~ group, two_sample_long)
```

### 

The default assumes unequal variances.

To change that, we need `var.equal = TRUE` argument. 

```{r ind7, exercise = TRUE}
t.test(first_walk ~ group, two_sample_long,
       var.equal = TRUE)
```


```{r quiz3, echo = FALSE}
quiz(question_text("How can we know from the output if equal variances were assumed or not?",
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3),
  question_text("What is the probability of observing a sample mean difference this large if there is actually no mean difference in the population?",
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3),
	question_text("How do we interpret the p-value, sample mean difference, and CI of this result? Is the result statistically significant?",
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
)
```

### 

Let's make the output nice, one last time. 

```{r ind8, exercise = TRUE}
clean_2sample <- tidy(t.test(
  first_walk ~ group, two_sample_long,
       var.equal = TRUE)) |> 
  rename(`Mean difference` = estimate, 
         `t-statistic` = statistic,
         `Control mean` = estimate1, `Therapy Mean` = estimate2,
         DF = parameter ) |> select(-alternative)

nice_res<-clean_2sample|> nice_table() 
nice_res
#export to word
flextable::save_as_docx(nice_res, path = "two_sample.docx")
```

### APA style write-up

We conducted an independent samples t-test to examine mean differences between the intervention and control groups. Levene's test was not significant ($\chi^2(1, 98) = 1.44, p = 0.233$), suggesting equal variances can be assumed across groups. The t-test indicated significant mean differences between the groups ($t(98) = 1.99, p = 0.049)$, with an estimated mean difference of 1.11 months (95% CI: 0.00, 2.22). Figure 1 presents the distribution of walking time in months between the groups, illustrating the small difference between the two. 

```{r ind9, exercise = TRUE, fig.height=5.5}
library(ggpubr)
ggboxplot(two_sample_long, x = "group", y = "first_walk",
          ylab = "Age at first walking", add = "jitter",
          title = "Figure 1. Time to first walking by treatment group.", 
          subtitle = "The treatment group walks sooner, but the difference is small.",
          color = "group") + #bonus! from ggpubr:
  stat_compare_means(method = "t.test", 
                     label="p.signif", #"p.format" will use number
                     comparisons = list(c("control", "intervention")))
```

see documentation on `ggpubr`:[https://rpkgs.datanovia.com/ggpubr/]( https://rpkgs.datanovia.com/ggpubr/)

```{r download-answers, child = system.file("child_documents/download_answers.Rmd", package = "tutorial.helpers")}
```
